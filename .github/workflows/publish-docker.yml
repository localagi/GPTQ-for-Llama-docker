name: Publish Docker images

on:
  release:
    types: [published]
  workflow_dispatch:

permissions:
  packages: write
  contents: read

jobs:
        
  build-llama-gptq:
    uses: localagi/ai-pipeline/.github/workflows/operation-docker-build-publish.yml@v3
    with:
      registry-repo-name: gptq-for-llama
      context-repository: ${{ matrix.variant }}/GPTQ-for-LLaMa
      context-repository-ref: ${{ matrix.branch }}
      tags: |
        type=schedule
        type=ref,event=branch
        type=semver,pattern={{version}}
      flavor: |
        suffix=-${{ matrix.variant }}-cc-${{ matrix.compute_version_id || matrix.compute_version }}
      build-args: |
        TORCH_CUDA_ARCH_LIST=${{ matrix.compute_version }}
    secrets: inherit
    strategy:
      fail-fast: false
      matrix:
        # see https://pytorch.org/docs/stable/cpp_extension.html
        compute_version: ["6.1", "8.6+PTX"]
        variant: [ oobabooga, qwopqwop200 ]
        include:
          - compute_version_id: 8.6PTX #fix invalid docker name
            compute_version: "8.6+PTX"
            # see https://github.com/oobabooga/text-generation-webui/blob/master/docs/GPTQ-models-(4-bit-mode).md
          - variant: oobabooga
            branch: cuda
          - variant: qwopqwop200
            branch: cuda
#          - variant: triton
#            repo: https://github.com/qwopqwop200/GPTQ-for-LLaMa.git
#            branch: "triton"
# https://github.com/PanQiWei/AutoGPTQ-TRiton is replacement
