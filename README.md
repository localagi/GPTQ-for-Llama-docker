# GPTQ-for-Llama-docker

Sophisticated docker builds for parent projects [oobabooga/GPTQ-for-Llama](https://github.com/oobabooga/GPTQ-for-LLaMa) and [qwopqwop200/GPTQ-for-Llama](https://github.com/qwopqwop200/GPTQ-for-LLaMa)

![example workflow](https://github.com/localagi/GPTQ-for-Llama-docker/actions/workflows/publish-docker.yml/badge.svg?branch=main)

Easy setup. Compatible. Tweakable. Scaleable.

# WIP

`source alias.gptq-for-llama`

`gptq-for-llama convert_llama_weights_to_hf.py --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir ./llama-hf`

